{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.input_size = 504\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 504,\n",
    "                            hidden_size = 64,\n",
    "                            num_layers = 1,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, time_series, batch_size=1):\n",
    "        \n",
    "        batch_size = time_series.shape[0]\n",
    "        time_series_size = time_series.shape[1]\n",
    "        lstm_dim = (batch_size, 1, time_series_size)\n",
    "        \n",
    "        tensor_series = time_series.unsqueeze(0).view(lstm_dim)\n",
    "\n",
    "        output, hidden_state = self.lstm(tensor_series)\n",
    "        \n",
    "        out = self.linear(output.view(time_series.shape[0], -1))\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis(torch.randn(3, 504)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.input_size = 32\n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = 32,\n",
    "                            hidden_size = 256,\n",
    "                            num_layers = 1,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(256, 504)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        \n",
    "        batch_size = noise.shape[0]\n",
    "        noise_size = noise.shape[1]\n",
    "        lstm_dim = (batch_size, 1, noise_size)\n",
    "        \n",
    "        tensor_noise = noise.unsqueeze(0).view(lstm_dim)\n",
    "\n",
    "        output, hidden_state = self.lstm(tensor_noise)\n",
    "        \n",
    "        out = self.linear(output.view(noise.shape[0], -1))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 504])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(torch.randn(12, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmGAN():\n",
    "\n",
    "    def __init__(self, input_data, epochs=5000, lambda_gp=5, generator_path='generator_lstm.pth', discriminator_path='discriminator_lstm.pth'):\n",
    "\n",
    "        self.cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "        self.generator_path = generator_path\n",
    "        self.discriminator_path = discriminator_path\n",
    "\n",
    "        self.generator, self.optimizer_generator = self._init_generator_(\n",
    "            generator_path)\n",
    "        self.discriminator, self.optimizer_discriminator = self._init_discriminator_(\n",
    "            discriminator_path)\n",
    "\n",
    "        self.last_epoch_saved = self._get_last_epoch_(generator_path)\n",
    "\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.epochs = epochs\n",
    "        self.noise_dim = 32\n",
    "        self.batch_size = 256\n",
    "\n",
    "        self.dataloader = self._get_tesor_(input_data)\n",
    "\n",
    "    def _get_tesor_(self, input_data):\n",
    "        input_tensor = torch.tensor(input_data.T.values).to(self.cuda)\n",
    "\n",
    "        means = input_tensor.mean(0, keepdim=True)\n",
    "        deviations = input_tensor.std(0, keepdim=True)\n",
    "\n",
    "        input_tensor_scaled = (input_tensor - means) / (deviations + 0.000001)\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            input_tensor_scaled, batch_size=self.batch_size)\n",
    "\n",
    "        assert input_tensor_scaled.shape[1] == 504\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def _init_generator_(self, model_path):    \n",
    "\n",
    "        generator = Generator().to(self.cuda)\n",
    "        optimizer_generator = torch.optim.Adam(generator.parameters())\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "\n",
    "            print('initializing generator')\n",
    "\n",
    "            checkpoint_generator = torch.load(model_path)\n",
    "            generator.load_state_dict(checkpoint_generator['model_state_dict'])\n",
    "            optimizer_generator.load_state_dict(\n",
    "                checkpoint_generator['optimizer_state_dict'])\n",
    "\n",
    "        return (generator, optimizer_generator)\n",
    "\n",
    "    def _init_discriminator_(self, model_path):       \n",
    "\n",
    "        discriminator = Discriminator().to(self.cuda)\n",
    "        optimizer_discriminator = torch.optim.Adam(discriminator.parameters())\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "\n",
    "            print('initializing discriminator')\n",
    "\n",
    "            checkpoint_discriminator = torch.load(model_path)\n",
    "            discriminator.load_state_dict(\n",
    "                checkpoint_discriminator['model_state_dict'])\n",
    "            optimizer_discriminator.load_state_dict(\n",
    "                checkpoint_discriminator['optimizer_state_dict'])\n",
    "\n",
    "        return (discriminator, optimizer_discriminator)\n",
    "\n",
    "    def _get_last_epoch_(self, model_path='generator.pth'):\n",
    "\n",
    "        last_epoch_saved = 0\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            last_epoch_saved = checkpoint['epoch']\n",
    "\n",
    "        return last_epoch_saved\n",
    "\n",
    "    def _compute_gradient_penalty_(self, discriminator, real_samples, fake_samples, batch_size):\n",
    "\n",
    "        alpha = self.Tensor(np.random.normal(0, 1, (batch_size, 504)))\n",
    "        \n",
    "        print(alpha.shape)\n",
    "        print(real_samples.shape)\n",
    "        print(fake_samples.shape)\n",
    "\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        \n",
    "        d_interpolates = discriminator(interpolates)\n",
    "        \n",
    "        fake = Variable(self.Tensor(1, batch_size, 1, 1).fill_(1.0), requires_grad=False)\n",
    "\n",
    "        gradients = autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def _train_report_(self, epoch, batch, discriminator_loss, generator_loss):\n",
    "\n",
    "        show_train_step = epoch % 50 == 0 and batch == 0\n",
    "        if show_train_step:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, self.epochs, batch, len(self.dataloader), discriminator_loss.item(), generator_loss.item())\n",
    "            )\n",
    "\n",
    "        show_generated_time_serie = epoch % 100 == 0 and batch == 0\n",
    "        if show_generated_time_serie:\n",
    "            noise = Variable(self.Tensor(np.random.normal(\n",
    "                0, 1, (self.batch_size, self.noise_dim))))\n",
    "            fake_ts = self.generator.forward(noise.unsqueeze(0))\n",
    "            plt.plot(fake_ts.cpu().detach().numpy().squeeze()[0])\n",
    "            plt.show()\n",
    "\n",
    "    def _save_model_(self, epoch, batch, generator_loss, discriminator_loss):\n",
    "        will_save_model = epoch % 500 == 0 and epoch != 0 and batch == 0\n",
    "        if will_save_model:\n",
    "            print('Saving model')\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.generator.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer_generator.state_dict(),\n",
    "                'loss': generator_loss,\n",
    "            }, self.generator_path)\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.discriminator.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer_discriminator.state_dict(),\n",
    "                'loss': discriminator_loss,\n",
    "            }, self.discriminator_path)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.last_epoch_saved, self.epochs)):\n",
    "            for batch, time_serie in enumerate(self.dataloader):\n",
    "\n",
    "                batch_size_epoch = time_serie.shape[0]\n",
    "                real_time_serie = time_serie\n",
    "\n",
    "                self.optimizer_discriminator.zero_grad()\n",
    "\n",
    "                noise = Variable(self.Tensor(np.random.normal(0, 1, (batch_size_epoch, self.noise_dim)))).to(self.cuda)\n",
    "\n",
    "                fake_time_serie = self.generator(noise)\n",
    "\n",
    "                fake_validity = self.discriminator(fake_time_serie.float())\n",
    "                real_validity = self.discriminator(real_time_serie.float())\n",
    "\n",
    "                gradient_penalty = self._compute_gradient_penalty_(\n",
    "                    self.discriminator, real_validity, fake_validity, batch_size_epoch)\n",
    "\n",
    "                discriminator_loss = -torch.mean(real_validity) + torch.mean(\n",
    "                    fake_validity) + self.lambda_gp * gradient_penalty\n",
    "\n",
    "                discriminator_loss.backward()\n",
    "                self.optimizer_discriminator.step()\n",
    "\n",
    "                self.optimizer_generator.zero_grad()\n",
    "\n",
    "                will_train_generator = batch % 10 == 0\n",
    "                if will_train_generator:\n",
    "\n",
    "                    fake_time_serie = self.generator(noise)\n",
    "\n",
    "                    fake_validity = self.discriminator(fake_time_serie.float())\n",
    "\n",
    "                    generator_loss = -torch.mean(fake_validity)\n",
    "\n",
    "                    generator_loss.backward()\n",
    "                    self.optimizer_generator.step()\n",
    "\n",
    "                    self._train_report_(\n",
    "                        epoch, batch, discriminator_loss, generator_loss)\n",
    "                    self._save_model_(\n",
    "                        epoch, batch, generator_loss, discriminator_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv('returns_for_gan.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.index = returns.Date\n",
    "returns.drop('Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BetaShares Gold Bullion Currency Hedged</th>\n",
       "      <th>Vanguard Australian Shares</th>\n",
       "      <th>iShares S&amp;P/ASX Dividend Opportunities</th>\n",
       "      <th>Ishares Core S&amp;P/Asx 200</th>\n",
       "      <th>Vanguard Australian Property Securities</th>\n",
       "      <th>Vanguard Australian Shares High Yield</th>\n",
       "      <th>ETFS Physical Gold</th>\n",
       "      <th>BetaShares US Dollar</th>\n",
       "      <th>Vanguard All World Ex US Shares</th>\n",
       "      <th>iSharesGlobal 100</th>\n",
       "      <th>...</th>\n",
       "      <th>Ennakl Automobiles SA</th>\n",
       "      <th>Poulina Group Holding</th>\n",
       "      <th>Societe Frigorifique Et Brasserie</th>\n",
       "      <th>Societe Moderne De Ceramique</th>\n",
       "      <th>Société Tunisienne d'Entreprises de Télécommunication</th>\n",
       "      <th>Societe Tunisienne De L'Air</th>\n",
       "      <th>Telnet Holding</th>\n",
       "      <th>Societe Tunisie Profiles Aluminium</th>\n",
       "      <th>Union Internationale De Banques</th>\n",
       "      <th>Societe Tunisienne De Verreries</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-12-02</th>\n",
       "      <td>-0.001142</td>\n",
       "      <td>0.011482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.009771</td>\n",
       "      <td>-0.001932</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>-0.001692</td>\n",
       "      <td>-0.041436</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007865</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.026940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-12-05</th>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.011712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.011275</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>-0.009456</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>-0.006359</td>\n",
       "      <td>-0.005917</td>\n",
       "      <td>-0.006689</td>\n",
       "      <td>-0.004918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-12-06</th>\n",
       "      <td>-0.018846</td>\n",
       "      <td>-0.012823</td>\n",
       "      <td>-0.014085</td>\n",
       "      <td>-0.014894</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>-0.006269</td>\n",
       "      <td>-0.013820</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>-0.003878</td>\n",
       "      <td>-0.008690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>-0.021480</td>\n",
       "      <td>-0.018613</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>-0.001065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-12-07</th>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.009719</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-0.010215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014308</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-12-08</th>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>-0.005682</td>\n",
       "      <td>-0.002139</td>\n",
       "      <td>-0.000845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>-0.000346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009677</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>-0.017857</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>-0.001613</td>\n",
       "      <td>-0.001064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BetaShares Gold Bullion Currency Hedged  \\\n",
       "Date                                                  \n",
       "2011-12-02                                -0.001142   \n",
       "2011-12-05                                 0.001144   \n",
       "2011-12-06                                -0.018846   \n",
       "2011-12-07                                 0.011059   \n",
       "2011-12-08                                 0.004606   \n",
       "\n",
       "            Vanguard Australian Shares  \\\n",
       "Date                                     \n",
       "2011-12-02                    0.011482   \n",
       "2011-12-05                    0.011712   \n",
       "2011-12-06                   -0.012823   \n",
       "2011-12-07                    0.003608   \n",
       "2011-12-08                    0.000539   \n",
       "\n",
       "            iShares S&P/ASX Dividend Opportunities  Ishares Core S&P/Asx 200  \\\n",
       "Date                                                                           \n",
       "2011-12-02                                0.000000                  0.013609   \n",
       "2011-12-05                                0.000000                  0.009667   \n",
       "2011-12-06                               -0.014085                 -0.014894   \n",
       "2011-12-07                                0.005714                  0.009719   \n",
       "2011-12-08                               -0.005682                 -0.002139   \n",
       "\n",
       "            Vanguard Australian Property Securities  \\\n",
       "Date                                                  \n",
       "2011-12-02                                 0.002134   \n",
       "2011-12-05                                 0.014696   \n",
       "2011-12-06                                -0.000840   \n",
       "2011-12-07                                -0.005882   \n",
       "2011-12-08                                -0.000845   \n",
       "\n",
       "            Vanguard Australian Shares High Yield  ETFS Physical Gold  \\\n",
       "Date                                                                    \n",
       "2011-12-02                               0.009771           -0.001932   \n",
       "2011-12-05                               0.018118            0.002480   \n",
       "2011-12-06                              -0.006269           -0.013820   \n",
       "2011-12-07                               0.000407           -0.000857   \n",
       "2011-12-08                               0.000000            0.005022   \n",
       "\n",
       "            BetaShares US Dollar  Vanguard All World Ex US Shares  \\\n",
       "Date                                                                \n",
       "2011-12-02              0.003093                        -0.002933   \n",
       "2011-12-05              0.002055                         0.011275   \n",
       "2011-12-06              0.004103                        -0.003878   \n",
       "2011-12-07             -0.010215                         0.000000   \n",
       "2011-12-08              0.001032                         0.006083   \n",
       "\n",
       "            iSharesGlobal 100  ...  Ennakl Automobiles SA  \\\n",
       "Date                           ...                          \n",
       "2011-12-02           0.006329  ...               0.008772   \n",
       "2011-12-05           0.005241  ...              -0.010870   \n",
       "2011-12-06          -0.008690  ...              -0.005495   \n",
       "2011-12-07           0.012973  ...               0.019890   \n",
       "2011-12-08          -0.000346  ...              -0.002167   \n",
       "\n",
       "            Poulina Group Holding  Societe Frigorifique Et Brasserie  \\\n",
       "Date                                                                   \n",
       "2011-12-02               0.007143                          -0.001692   \n",
       "2011-12-05              -0.009456                           0.001695   \n",
       "2011-12-06              -0.021480                          -0.018613   \n",
       "2011-12-07               0.000000                           0.017241   \n",
       "2011-12-08              -0.006098                           0.002542   \n",
       "\n",
       "            Societe Moderne De Ceramique  \\\n",
       "Date                                       \n",
       "2011-12-02                     -0.041436   \n",
       "2011-12-05                      0.020173   \n",
       "2011-12-06                      0.002825   \n",
       "2011-12-07                      0.000000   \n",
       "2011-12-08                      0.000000   \n",
       "\n",
       "            Société Tunisienne d'Entreprises de Télécommunication  \\\n",
       "Date                                                                \n",
       "2011-12-02                                           0.014516       \n",
       "2011-12-05                                          -0.006359       \n",
       "2011-12-06                                           0.006400       \n",
       "2011-12-07                                          -0.014308       \n",
       "2011-12-08                                          -0.009677       \n",
       "\n",
       "            Societe Tunisienne De L'Air  Telnet Holding  \\\n",
       "Date                                                      \n",
       "2011-12-02                     0.000000        0.007865   \n",
       "2011-12-05                    -0.005917       -0.006689   \n",
       "2011-12-06                    -0.005952       -0.012346   \n",
       "2011-12-07                     0.005988        0.018182   \n",
       "2011-12-08                     0.011905       -0.017857   \n",
       "\n",
       "            Societe Tunisie Profiles Aluminium  \\\n",
       "Date                                             \n",
       "2011-12-02                            0.008264   \n",
       "2011-12-05                           -0.004918   \n",
       "2011-12-06                           -0.011532   \n",
       "2011-12-07                            0.000000   \n",
       "2011-12-08                            0.008333   \n",
       "\n",
       "            Union Internationale De Banques  Societe Tunisienne De Verreries  \n",
       "Date                                                                          \n",
       "2011-12-02                        -0.003214                         0.026940  \n",
       "2011-12-05                         0.000000                        -0.014690  \n",
       "2011-12-06                        -0.000537                        -0.001065  \n",
       "2011-12-07                         0.000000                         0.002132  \n",
       "2011-12-08                        -0.001613                        -0.001064  \n",
       "\n",
       "[5 rows x 14610 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_gan = returns.iloc[-504:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_gan = LstmGAN(input_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 504])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (504) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-bd5e4be0442f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-c9958b930ae2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 gradient_penalty = self._compute_gradient_penalty_(\n\u001b[0;32m--> 164\u001b[0;31m                     self.discriminator, real_validity, fake_validity, batch_size_epoch)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 discriminator_loss = -torch.mean(real_validity) + torch.mean(\n",
      "\u001b[0;32m<ipython-input-184-c9958b930ae2>\u001b[0m in \u001b[0;36m_compute_gradient_penalty_\u001b[0;34m(self, discriminator, real_samples, fake_samples, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0minterpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfake_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0md_interpolates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterpolates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (504) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "lstm_gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
